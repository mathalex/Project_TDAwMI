{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4c34e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nibabel.loadsave import ImageFileError\n",
    "\n",
    "from nilearn.image import load_img\n",
    "from nilearn import datasets\n",
    "from nilearn.maskers import NiftiMasker, NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nilearn import plotting\n",
    "\n",
    "from itertools import combinations, product\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy.stats import spearmanr, entropy\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d, uniform_filter1d, maximum_filter1d, minimum_filter1d\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.linalg import logm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import ripser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf19ac9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((956, 226, 116), (956, 116, 116), (956,))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_raw = np.load(\"T_subjects_1,5_nsd_vs_rest.npy\")[:, 1]\n",
    "X_raw = np.load(\"X_subjects_1,5_nsd_vs_rest.npy\")[:, 1]\n",
    "y = np.load(\"y_subjects_1,5_nsd_vs_rest.npy\")[1]\n",
    "T_raw.shape, X_raw.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d088f926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53],\n",
       "  'visual cortex (12 regions)'),\n",
       " ([0, 1, 6, 7, 18, 19, 56, 57, 62, 63, 68, 69],\n",
       "  'sensorimotor network (SMN) (12 regions)'),\n",
       " ([28, 29, 30, 31, 84, 85], 'dorsal attention network (DAN) (6 regions)'),\n",
       " ([32, 33, 34, 35, 36, 37, 52, 53, 62, 63, 64, 65],\n",
       "  'ventral attention network (VAN) (12 regions)'),\n",
       " ([6, 7, 10, 11, 12, 13, 60, 61, 64, 65],\n",
       "  'frontoparietal network (FPN) (10 regions)'),\n",
       " ([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 80, 81, 82, 83, 84, 85, 86, 87],\n",
       "  'limbic system (18 regions)'),\n",
       " ([32, 33, 34, 35, 36, 37, 52, 53, 62, 63, 64, 65],\n",
       "  'default mode network (DMN) (12 regions)')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visual cortex\n",
    "visual = list(range(42, 53+1))\n",
    "# sensorimotor network, somatomotor network (SMN)\n",
    "smn = [0, 1, 6, 7, 18, 19, 56, 57, 62, 63, 68, 69]\n",
    "# dorsal attention network (DAN), dorsal frontoparietal network (D-FPN)\n",
    "dan = [28, 29, 30, 31, 84, 85]\n",
    "# ventral attention network (VAN), ventral frontoparietal network (VFN), ventral attention system (VAS)\n",
    "van = [32, 33, 34, 35, 36, 37, 52, 53, 62, 63, 64, 65]\n",
    "# frontoparietal network (FPN), central executive network (CEN), lateral frontoparietal network (L-FPN)\n",
    "fpn = [6, 7, 10, 11, 12, 13, 60, 61, 64, 65]\n",
    "# limbic system, paleomammalian cortex\n",
    "limbic = list(range(30, 39+1)) + list(range(80, 87+1))\n",
    "# default mode network (DMN), default network, default state network, medial frontoparietal network (M-FPN)\n",
    "dmn = [22, 23, 34, 35, 36, 37, 38, 39, 64, 65, 66, 67]\n",
    "dmn = van\n",
    "\n",
    "brain_networks = [visual, smn, dan, van, fpn, limbic, dmn]\n",
    "brain_network_names = [\"visual cortex (12 regions)\",\n",
    "                       \"sensorimotor network (SMN) (12 regions)\",\n",
    "                       \"dorsal attention network (DAN) (6 regions)\",\n",
    "                       \"ventral attention network (VAN) (12 regions)\",\n",
    "                       \"frontoparietal network (FPN) (10 regions)\",\n",
    "                       \"limbic system (18 regions)\",\n",
    "                       \"default mode network (DMN) (12 regions)\"\n",
    "                      ]\n",
    "brain_short_network_names = [\"visual (12)\", \"SMN (12)\", \"DAN (6)\", \"VAN (12)\", \"FPN (10)\", \"limbic (18)\", \"DMN (12)\"]\n",
    "brain_networks_with_names = list(zip(brain_networks, brain_network_names))\n",
    "brain_networks_with_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "98829abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussI(S):\n",
    "    return -0.5 * np.log(np.linalg.det(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "11e88e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_brain_network(T_raw, X_raw, brain_network):\n",
    "    bn_idx = np.ix_(np.arange(0, X_raw.shape[0]), brain_network, brain_network)\n",
    "    T_bn = T_raw[:, :, brain_network]\n",
    "    X_bn = X_raw[bn_idx]\n",
    "    return T_bn, X_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aeb7699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dionysus\n",
    "\n",
    "# def get_simplices_from_corrmat(T_bn, X_bn, k=2, modif=abs, check=lambda w: True):\n",
    "#     # to prune lightweight edges use check=lambda w: w > threshold\n",
    "#     # to prune negative correlations use modif=id, check=lambda w: w > 0\n",
    "#     n = X_bn.shape[0]\n",
    "#     simplices = {}\n",
    "#     for i in range(n):\n",
    "#         simplices[(i,)] = 0\n",
    "#         for j in range(i):\n",
    "#             w = modif(X_bn[j, i])\n",
    "#             if check(w):\n",
    "#                 simplices[(j, i)] = w\n",
    "#     for dim in range(2, k + 1):\n",
    "#         ...\n",
    "    \n",
    "    \n",
    "    \n",
    "# def get_simplices_functor_with_params(func, *args, **kwargs):\n",
    "#     return (lambda T_bn, X_bn: func(*args, **kwargs))\n",
    "\n",
    "# def get_persistence_diagram(simplices, reverse=True):\n",
    "#     filtration = dionysus.Filtration()\n",
    "#     for vertices, time in simplices:\n",
    "#         filtration.append(dionysus.Simplex(vertices, time))\n",
    "#     filtration.sort(reverse=reverse)\n",
    "\n",
    "#     diagram = dionysus.init_diagrams(dionysus.homology_persistence(filtration), filtration)\n",
    "#     return diagram\n",
    "    \n",
    "# def get_persistence_diagram_functor_with_params(func, *args, **kwargs):\n",
    "#     return (lambda simplices: func(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "02213f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_persistence_diagram_from_corrmat(T_bn, X_bn, dim=2, threshold=-2):\n",
    "    return ripser.ripser(1 - np.abs(X_bn), thresh=1 - threshold, maxdim=dim, distance_matrix=True)['dgms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "016da8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.        , 0.12529169],\n",
       "        [0.        , 0.22025172],\n",
       "        [0.        , 0.26729095],\n",
       "        [0.        , 0.28290591],\n",
       "        [0.        , 0.35668072],\n",
       "        [0.        , 0.3819766 ],\n",
       "        [0.        , 0.40466523],\n",
       "        [0.        , 0.44850785],\n",
       "        [0.        , 0.45469671],\n",
       "        [0.        , 0.4919467 ],\n",
       "        [0.        , 0.51505613],\n",
       "        [0.        ,        inf]]),\n",
       " array([[0.40878159, 0.43009564]]),\n",
       " array([], shape=(0, 2), dtype=float64)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_dmn, X_dmn = apply_brain_network(T_raw, X_raw, dmn)\n",
    "persistence_diagram = get_persistence_diagram_from_corrmat(T_dmn[0], X_dmn[0])\n",
    "persistence_diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2eb401ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "l0, l1, l2 = [], [], []\n",
    "for i in range(len(T_dmn)):\n",
    "    persistence_diagram = get_persistence_diagram_from_corrmat(T_dmn[i], X_dmn[i])\n",
    "    l0.append(len(persistence_diagram[0]))\n",
    "    l1.append(len(persistence_diagram[1]))\n",
    "    l2.append(len(persistence_diagram[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a721e49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12, 956)]\n",
      "[(0, 139), (1, 340), (2, 298), (3, 127), (4, 47), (5, 5)]\n",
      "[(0, 829), (1, 115), (2, 10), (3, 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(sorted(Counter(l0).items()))\n",
    "print(sorted(Counter(l1).items()))\n",
    "print(sorted(Counter(l2).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c2aba742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 956/956 [00:00<00:00, 2063.69it/s]\n"
     ]
    }
   ],
   "source": [
    "def conv_pd(diagrams):\n",
    "    pd = np.zeros((0, 3))\n",
    "\n",
    "    for k, diagram_k in enumerate(diagrams):\n",
    "        diagram_k = diagram_k[~np.isinf(diagram_k).any(axis=1)] # filter infs  \n",
    "        diagram_k = np.concatenate((diagram_k, k * np.ones((diagram_k.shape[0], 1))), axis=1)\n",
    "        pd = np.concatenate((pd, diagram_k))\n",
    "\n",
    "    return pd\n",
    "\n",
    "X = []\n",
    "\n",
    "for x_pc in tqdm(X_dmn):\n",
    "    diagram = conv_pd(get_persistence_diagram_from_corrmat(_, x_pc))\n",
    "    X.append(diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8d246a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ripser import lower_star_img\n",
    "from ripser import Rips\n",
    "\n",
    "import persim\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "814aa62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orbit2kDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "def collate_fn(data):\n",
    "    \n",
    "    tmp_pd, _ = data[0]\n",
    "    \n",
    "    n_batch = len(data)\n",
    "    n_features_pd = tmp_pd.shape[1]\n",
    "    n_points_pd = max(len(pd) for pd, _ in data)\n",
    "    inputs_pd = np.zeros((n_batch, n_points_pd, n_features_pd), dtype=float)\n",
    "    labels = np.zeros(len(data))\n",
    "    \n",
    "    for i, (pd, label) in enumerate(data):\n",
    "        inputs_pd[i][:len(pd)] = pd\n",
    "        labels[i] = label\n",
    "    \n",
    "    return torch.Tensor(inputs_pd), torch.Tensor(labels).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "15919c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSets(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden_enc, n_out_enc, n_hidden_dec=16, n_out_dec=2):\n",
    "        super(DeepSets, self).__init__()\n",
    "        self.encoder = Encoder(n_in, n_hidden_enc, n_out_enc)\n",
    "        self.decoder = MLP(n_out_enc, n_hidden_dec, n_out_dec)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        z_enc = self.encoder(X)\n",
    "        z = self.decoder(z_enc)\n",
    "        return z\n",
    "    \n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = Linear(n_in, n_hidden)\n",
    "        self.linear2 = Linear(n_hidden, n_out)\n",
    "        self.bn = torch.nn.BatchNorm1d(n_out)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = relu(self.linear1(X))\n",
    "        X = self.linear2(X)\n",
    "        return X\n",
    "    \n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.mlp = MLP(n_in, n_hidden, n_out)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.mlp(X)\n",
    "        x = X.mean(dim=1) # aggregation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1dd8c015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 0.6936 0.5157 0.4648\n",
      "  1 0.6932 0.5157 0.4648\n",
      "  2 0.6929 0.5157 0.4648\n",
      "  3 0.6931 0.5157 0.4648\n",
      "  4 0.6927 0.5157 0.4648\n",
      "  5 0.6927 0.5157 0.4648\n",
      "  6 0.6927 0.5157 0.4648\n",
      "  7 0.6926 0.5157 0.4648\n",
      "  8 0.6925 0.5157 0.4648\n",
      "  9 0.6924 0.5157 0.4648\n",
      " 10 0.6920 0.5157 0.4648\n",
      " 11 0.6919 0.5157 0.4648\n",
      " 12 0.6917 0.5157 0.4648\n",
      " 13 0.6915 0.5157 0.4648\n",
      " 14 0.6913 0.5157 0.4648\n",
      " 15 0.6910 0.5157 0.4648\n",
      " 16 0.6909 0.5157 0.4648\n",
      " 17 0.6907 0.5157 0.4648\n",
      " 18 0.6901 0.5157 0.4648\n",
      " 19 0.6901 0.5157 0.4648\n",
      " 20 0.6894 0.5157 0.4648\n",
      " 21 0.6891 0.5157 0.4648\n",
      " 22 0.6885 0.5229 0.4648\n",
      " 23 0.6878 0.5357 0.4922\n",
      " 24 0.6872 0.5386 0.4883\n",
      " 25 0.6871 0.5514 0.5000\n",
      " 26 0.6861 0.5543 0.5391\n",
      " 27 0.6851 0.5543 0.5156\n",
      " 28 0.6851 0.5457 0.5391\n",
      " 29 0.6838 0.5514 0.5547\n",
      " 30 0.6841 0.5643 0.5742\n",
      " 31 0.6831 0.5686 0.5938\n",
      " 32 0.6829 0.5614 0.5703\n",
      " 33 0.6826 0.5843 0.6055\n",
      " 34 0.6819 0.5829 0.5977\n",
      " 35 0.6823 0.5743 0.6055\n",
      " 36 0.6813 0.5843 0.5938\n",
      " 37 0.6810 0.5843 0.6055\n",
      " 38 0.6804 0.5757 0.5898\n",
      " 39 0.6805 0.5771 0.6211\n",
      " 40 0.6806 0.5729 0.6016\n",
      " 41 0.6797 0.5757 0.6133\n",
      " 42 0.6791 0.5814 0.6133\n",
      " 43 0.6790 0.5657 0.6094\n",
      " 44 0.6786 0.5786 0.5977\n",
      " 45 0.6804 0.5829 0.6250\n",
      " 46 0.6792 0.5714 0.5977\n",
      " 47 0.6793 0.5714 0.6250\n",
      " 48 0.6788 0.5714 0.6250\n",
      " 49 0.6790 0.5829 0.6250\n",
      " 50 0.6781 0.5714 0.6172\n",
      " 51 0.6778 0.5814 0.6172\n",
      " 52 0.6799 0.5829 0.6211\n",
      " 53 0.6806 0.5629 0.6250\n",
      " 54 0.6784 0.5757 0.6250\n",
      " 55 0.6783 0.5757 0.6211\n",
      " 56 0.6779 0.5786 0.6172\n",
      " 57 0.6783 0.5771 0.6211\n",
      " 58 0.6770 0.5800 0.6211\n",
      " 59 0.6783 0.5786 0.6289\n",
      " 60 0.6773 0.5786 0.6328\n",
      " 61 0.6779 0.5857 0.6133\n",
      " 62 0.6778 0.5900 0.6250\n",
      " 63 0.6788 0.5771 0.6250\n",
      " 64 0.6763 0.5800 0.6172\n",
      " 65 0.6772 0.5657 0.6289\n",
      " 66 0.6783 0.5800 0.6172\n",
      " 67 0.6773 0.5771 0.6172\n",
      " 68 0.6777 0.5771 0.6250\n",
      " 69 0.6767 0.5757 0.6328\n",
      " 70 0.6784 0.5714 0.6445\n",
      " 71 0.6770 0.5743 0.6172\n",
      " 72 0.6779 0.5729 0.6328\n",
      " 73 0.6761 0.5714 0.6406\n",
      " 74 0.6774 0.5829 0.6172\n",
      " 75 0.6777 0.5814 0.6328\n",
      " 76 0.6774 0.5871 0.6211\n",
      " 77 0.6765 0.5757 0.6250\n",
      " 78 0.6762 0.5786 0.6289\n",
      " 79 0.6783 0.5871 0.6250\n",
      " 80 0.6765 0.5714 0.6250\n",
      " 81 0.6771 0.5886 0.6328\n",
      " 82 0.6765 0.5729 0.6289\n",
      " 83 0.6771 0.5657 0.6133\n",
      " 84 0.6788 0.5829 0.6094\n",
      " 85 0.6780 0.5857 0.6289\n",
      " 86 0.6761 0.5814 0.6250\n",
      " 87 0.6766 0.5757 0.6211\n",
      " 88 0.6781 0.5814 0.6172\n",
      " 89 0.6752 0.5771 0.6406\n",
      " 90 0.6767 0.5786 0.6445\n",
      " 91 0.6753 0.5729 0.6133\n",
      " 92 0.6778 0.5743 0.6250\n",
      " 93 0.6755 0.5771 0.6367\n",
      " 94 0.6753 0.5814 0.6328\n",
      " 95 0.6785 0.5871 0.6289\n",
      " 96 0.6757 0.5729 0.6172\n",
      " 97 0.6750 0.5800 0.6328\n",
      " 98 0.6782 0.5757 0.6406\n",
      " 99 0.6748 0.5814 0.6172\n",
      "0.64453125\n",
      "\n",
      "  0 0.6933 0.5029 0.4844\n",
      "  1 0.6930 0.5029 0.4844\n",
      "  2 0.6932 0.5757 0.5898\n",
      "  3 0.6928 0.5029 0.4844\n",
      "  4 0.6928 0.5057 0.4961\n",
      "  5 0.6925 0.5371 0.5469\n",
      "  6 0.6923 0.5471 0.5273\n",
      "  7 0.6923 0.5757 0.5781\n",
      "  8 0.6922 0.5543 0.5352\n",
      "  9 0.6920 0.5843 0.5547\n",
      " 10 0.6919 0.5786 0.5898\n",
      " 11 0.6917 0.5657 0.5312\n",
      " 12 0.6914 0.5743 0.5820\n",
      " 13 0.6912 0.5757 0.5898\n",
      " 14 0.6908 0.5914 0.5625\n",
      " 15 0.6911 0.5743 0.5742\n",
      " 16 0.6905 0.5743 0.5547\n",
      " 17 0.6900 0.5857 0.5508\n",
      " 18 0.6896 0.5714 0.5586\n",
      " 19 0.6893 0.5843 0.5820\n",
      " 20 0.6888 0.5729 0.5664\n",
      " 21 0.6887 0.5686 0.5586\n",
      " 22 0.6883 0.5714 0.5703\n",
      " 23 0.6876 0.5829 0.5547\n",
      " 24 0.6873 0.5800 0.5859\n",
      " 25 0.6872 0.5729 0.5742\n",
      " 26 0.6861 0.5757 0.5664\n",
      " 27 0.6856 0.5786 0.5586\n",
      " 28 0.6855 0.5829 0.5742\n",
      " 29 0.6846 0.5786 0.5820\n",
      " 30 0.6839 0.5814 0.5664\n",
      " 31 0.6839 0.5886 0.5664\n",
      " 32 0.6824 0.5900 0.5820\n",
      " 33 0.6822 0.5786 0.5703\n",
      " 34 0.6824 0.5900 0.5625\n",
      " 35 0.6823 0.5800 0.5781\n",
      " 36 0.6807 0.5671 0.5547\n",
      " 37 0.6811 0.5814 0.5703\n",
      " 38 0.6801 0.5757 0.5664\n",
      " 39 0.6795 0.5843 0.5820\n",
      " 40 0.6801 0.5743 0.5703\n",
      " 41 0.6789 0.5743 0.5625\n",
      " 42 0.6784 0.5729 0.5859\n",
      " 43 0.6782 0.5800 0.5781\n",
      " 44 0.6799 0.5829 0.5469\n",
      " 45 0.6783 0.5771 0.5742\n",
      " 46 0.6767 0.5743 0.5820\n",
      " 47 0.6780 0.5800 0.5859\n",
      " 48 0.6767 0.5800 0.5742\n",
      " 49 0.6776 0.5814 0.5664\n",
      " 50 0.6774 0.5814 0.5664\n",
      " 51 0.6767 0.5843 0.5742\n",
      " 52 0.6764 0.5786 0.5820\n",
      " 53 0.6761 0.5857 0.5703\n",
      " 54 0.6767 0.5757 0.5898\n",
      " 55 0.6757 0.5786 0.5664\n",
      " 56 0.6747 0.5886 0.5898\n",
      " 57 0.6762 0.5814 0.5820\n",
      " 58 0.6758 0.5829 0.5781\n",
      " 59 0.6762 0.5757 0.5781\n",
      " 60 0.6753 0.5729 0.5898\n",
      " 61 0.6753 0.5771 0.5703\n",
      " 62 0.6753 0.5800 0.5820\n",
      " 63 0.6753 0.5757 0.5742\n",
      " 64 0.6758 0.5829 0.5820\n",
      " 65 0.6748 0.5786 0.5781\n",
      " 66 0.6749 0.5757 0.5859\n",
      " 67 0.6770 0.5843 0.5820\n",
      " 68 0.6759 0.5829 0.5664\n",
      " 69 0.6746 0.5886 0.5703\n",
      " 70 0.6752 0.5843 0.5820\n",
      " 71 0.6749 0.5786 0.5938\n",
      " 72 0.6754 0.5843 0.5781\n",
      " 73 0.6767 0.5800 0.5703\n",
      " 74 0.6760 0.5886 0.5781\n",
      " 75 0.6752 0.5671 0.5938\n",
      " 76 0.6747 0.5871 0.5781\n",
      " 77 0.6752 0.5814 0.5898\n",
      " 78 0.6758 0.5857 0.5898\n",
      " 79 0.6743 0.5843 0.5703\n",
      " 80 0.6747 0.5700 0.5859\n",
      " 81 0.6729 0.5714 0.5859\n",
      " 82 0.6747 0.5786 0.5742\n",
      " 83 0.6754 0.5686 0.5742\n",
      " 84 0.6728 0.5757 0.5820\n",
      " 85 0.6743 0.5814 0.5742\n",
      " 86 0.6742 0.5886 0.5859\n",
      " 87 0.6731 0.5743 0.5781\n",
      " 88 0.6738 0.5771 0.5820\n",
      " 89 0.6738 0.5843 0.5859\n",
      " 90 0.6736 0.5757 0.6055\n",
      " 91 0.6749 0.5843 0.5820\n",
      " 92 0.6740 0.5786 0.5742\n",
      " 93 0.6742 0.5871 0.5938\n",
      " 94 0.6729 0.5829 0.6016\n",
      " 95 0.6738 0.5743 0.5781\n",
      " 96 0.6748 0.5943 0.5898\n",
      " 97 0.6735 0.5857 0.6016\n",
      " 98 0.6728 0.5771 0.5898\n",
      " 99 0.6740 0.5829 0.5820\n",
      "0.60546875\n",
      "\n",
      "  0 0.6932 0.5100 0.4805\n",
      "  1 0.6926 0.5100 0.4805\n",
      "  2 0.6922 0.5100 0.4805\n",
      "  3 0.6919 0.5100 0.4805\n",
      "  4 0.6919 0.5100 0.4805\n",
      "  5 0.6910 0.5100 0.4805\n",
      "  6 0.6906 0.5100 0.4805\n",
      "  7 0.6901 0.5100 0.4805\n",
      "  8 0.6895 0.5171 0.4844\n",
      "  9 0.6886 0.5300 0.5117\n",
      " 10 0.6878 0.5157 0.4844\n",
      " 11 0.6873 0.5443 0.5156\n",
      " 12 0.6858 0.5529 0.5234\n",
      " 13 0.6851 0.5486 0.5234\n",
      " 14 0.6841 0.5786 0.5312\n",
      " 15 0.6829 0.5714 0.5156\n",
      " 16 0.6816 0.5814 0.5469\n",
      " 17 0.6805 0.6014 0.5234\n",
      " 18 0.6799 0.5971 0.5117\n",
      " 19 0.6773 0.5943 0.5391\n",
      " 20 0.6783 0.6043 0.5234\n",
      " 21 0.6763 0.6014 0.5273\n",
      " 22 0.6756 0.5957 0.5273\n",
      " 23 0.6739 0.6057 0.5195\n",
      " 24 0.6728 0.5971 0.5234\n",
      " 25 0.6725 0.6000 0.5312\n",
      " 26 0.6726 0.6071 0.5156\n",
      " 27 0.6743 0.6000 0.5391\n",
      " 28 0.6701 0.6043 0.5312\n",
      " 29 0.6707 0.5971 0.5469\n",
      " 30 0.6693 0.6000 0.5312\n",
      " 31 0.6688 0.6086 0.5312\n",
      " 32 0.6680 0.6071 0.5234\n",
      " 33 0.6684 0.6057 0.5156\n",
      " 34 0.6680 0.6000 0.5430\n",
      " 35 0.6709 0.6014 0.5117\n",
      " 36 0.6682 0.6000 0.5469\n",
      " 37 0.6680 0.5971 0.5273\n",
      " 38 0.6688 0.5986 0.5352\n",
      " 39 0.6661 0.6071 0.5312\n",
      " 40 0.6680 0.5971 0.5156\n",
      " 41 0.6671 0.6071 0.5352\n",
      " 42 0.6667 0.6000 0.5273\n",
      " 43 0.6672 0.6029 0.5273\n",
      " 44 0.6678 0.6029 0.5391\n",
      " 45 0.6666 0.5886 0.5430\n",
      " 46 0.6665 0.6029 0.5430\n",
      " 47 0.6644 0.5986 0.5273\n",
      " 48 0.6668 0.6114 0.5312\n",
      " 49 0.6679 0.5986 0.5391\n",
      " 50 0.6676 0.6014 0.5391\n",
      " 51 0.6672 0.5943 0.5234\n",
      " 52 0.6654 0.6029 0.5273\n",
      " 53 0.6658 0.5986 0.5312\n",
      " 54 0.6659 0.5971 0.5469\n",
      " 55 0.6655 0.5857 0.5234\n",
      " 56 0.6674 0.5986 0.5312\n",
      " 57 0.6674 0.5957 0.5547\n",
      " 58 0.6657 0.5929 0.5273\n",
      " 59 0.6654 0.5986 0.5352\n",
      " 60 0.6689 0.6043 0.5234\n",
      " 61 0.6649 0.5929 0.5508\n",
      " 62 0.6655 0.6000 0.5312\n",
      " 63 0.6689 0.5843 0.5430\n",
      " 64 0.6651 0.6043 0.5273\n",
      " 65 0.6675 0.5943 0.5469\n",
      " 66 0.6641 0.6029 0.5430\n",
      " 67 0.6676 0.5957 0.5352\n",
      " 68 0.6661 0.5986 0.5391\n",
      " 69 0.6656 0.5929 0.5273\n",
      " 70 0.6667 0.6043 0.5312\n",
      " 71 0.6671 0.6043 0.5508\n",
      " 72 0.6670 0.6000 0.5273\n",
      " 73 0.6652 0.6000 0.5352\n",
      " 74 0.6661 0.5971 0.5352\n",
      " 75 0.6639 0.5986 0.5430\n",
      " 76 0.6661 0.6000 0.5312\n",
      " 77 0.6646 0.6000 0.5352\n",
      " 78 0.6645 0.6043 0.5391\n",
      " 79 0.6686 0.5971 0.5508\n",
      " 80 0.6637 0.5986 0.5469\n",
      " 81 0.6646 0.5914 0.5234\n",
      " 82 0.6676 0.6029 0.5352\n",
      " 83 0.6656 0.5957 0.5391\n",
      " 84 0.6656 0.6014 0.5195\n",
      " 85 0.6685 0.6100 0.5234\n",
      " 86 0.6632 0.5914 0.5352\n",
      " 87 0.6667 0.5900 0.5430\n",
      " 88 0.6666 0.6014 0.5352\n",
      " 89 0.6639 0.5957 0.5312\n",
      " 90 0.6658 0.5914 0.5508\n",
      " 91 0.6643 0.6000 0.5273\n",
      " 92 0.6660 0.6000 0.5391\n",
      " 93 0.6640 0.6000 0.5391\n",
      " 94 0.6633 0.5971 0.5508\n",
      " 95 0.6663 0.5986 0.5508\n",
      " 96 0.6661 0.6071 0.5508\n",
      " 97 0.6655 0.5871 0.5586\n",
      " 98 0.6660 0.5914 0.5508\n",
      " 99 0.6636 0.5986 0.5391\n",
      "0.55859375\n",
      "\n",
      "0.60286 ± 0.03513\n",
      "CPU times: user 54.6 s, sys: 641 ms, total: 55.3 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_repeats = 3\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "\n",
    "n_train, n_test = 1600, 400\n",
    "\n",
    "history = np.zeros((n_repeats, n_epochs, 3))\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "dataset = Orbit2kDataset(X, y)\n",
    "\n",
    "ret = [0] * n_repeats\n",
    "for repeat_idx in range(n_repeats):\n",
    "    \n",
    "    # data init\n",
    "    dataset_train, dataset_test = random_split(dataset, [700, 256])\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader_test =  DataLoader(dataset_test, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # model init\n",
    "    model = DeepSets(n_in=3, n_hidden_enc=16, n_out_enc=8)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    #print(\"{:3} {:6} {:6} {:6}\".format(repeat_idx, \"Loss\", \"Train\", \"Test\"))\n",
    "    \n",
    "    mx = 0\n",
    "    for epoch_idx in range(n_epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        \n",
    "        loss_epoch = []\n",
    "        for batch in dataloader_train:\n",
    "            loss_batch = criterion(model(batch[0]), batch[1])\n",
    "            loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_epoch.append(loss_batch.detach())\n",
    "        \n",
    "        loss_epoch_mean = np.array(loss_epoch).mean()\n",
    "        history[repeat_idx,epoch_idx,0] = loss_epoch_mean\n",
    "        \n",
    "        # test\n",
    "        model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        for batch in dataloader_train:\n",
    "            y_hat = model(batch[0]).argmax(dim=1)\n",
    "            correct += int((y_hat == batch[1]).sum())\n",
    "        accuracy_train = correct / len(dataloader_train.dataset)\n",
    "        history[repeat_idx,epoch_idx,1] = accuracy_train\n",
    "\n",
    "        correct = 0\n",
    "        for batch in dataloader_test:\n",
    "            y_hat = model(batch[0]).argmax(dim=1)\n",
    "            correct += int((y_hat == batch[1]).sum())\n",
    "        accuracy_test = correct / len(dataloader_test.dataset)\n",
    "        history[repeat_idx,epoch_idx,2] = accuracy_test\n",
    "        \n",
    "        mx = max(mx, accuracy_test)\n",
    "        \n",
    "        print(\"{:3} {:.4f} {:.4f} {:.4f}\".format(epoch_idx, loss_epoch_mean, accuracy_train, accuracy_test))\n",
    "    print(mx)\n",
    "    ret[repeat_idx] = mx\n",
    "    print(\"\\r\")\n",
    "ret = np.array(ret)\n",
    "print(f\"{ret.mean():.5f} ± {ret.std():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "96482133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_test(T_raw, X_raw, y, brain_networks_with_names, get_simplices):\n",
    "    for brain_network, brain_network_name in brain_networks_with_names:\n",
    "        T_bn, X_bn = apply_brain_network(T_raw, X_raw, brain_network)\n",
    "        persistence_diagram = get_persistence_diagram(T_bn, X_bn)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d60fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
